{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND ATTEMPT\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh =  mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "  \n",
    "img = cv2.imread('filters/anonymous.png', cv2.IMREAD_UNCHANGED)\n",
    "image = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "# To improve performance.\n",
    "image.flags.writeable = False\n",
    "results =  face_mesh.process(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import faceBlendCommon as fbc\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_FACE_POINTS = False\n",
    "filters_config = {\n",
    "    'anonymous':\n",
    "        [{'path': \"filters/anonymous.png\",\n",
    "         'anno_path': \"annotations/anonymous_annotations.csv\",\n",
    "         'morph': True, 'animated': False, 'has_alpha': True}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLandmarks(img):\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    selected_keypoint_indices = [127, 93, 58, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379, 365, 288, 323, 356, 70, 63, 105, 66, 55,\n",
    "                 285, 296, 334, 293, 300, 168, 6, 195, 4, 64, 60, 94, 290, 439, 33, 160, 158, 173, 153, 144, 398, 385,\n",
    "                 387, 466, 373, 380, 61, 40, 39, 0, 269, 270, 291, 321, 405, 17, 181, 91, 78, 81, 13, 311, 306, 402, 14,\n",
    "                 178, 162, 54, 67, 10, 297, 284, 389]\n",
    " \n",
    "    height, width = img.shape[:-1]\n",
    " \n",
    "    with mp_face_mesh.FaceMesh(max_num_faces=1, static_image_mode=True, min_detection_confidence=0.5) as face_mesh:\n",
    " \n",
    "        results = face_mesh.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    " \n",
    "        if not results.multi_face_landmarks:\n",
    "            print('Face not detected!!!')\n",
    "            return 0\n",
    " \n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            values = np.array(face_landmarks.landmark)\n",
    "            face_keypnts = np.zeros((len(values), 2))\n",
    " \n",
    "            for idx,value in enumerate(values):\n",
    "                face_keypnts[idx][0] = value.x\n",
    "                face_keypnts[idx][1] = value.y\n",
    " \n",
    "            # Convert normalized points to image coordinates\n",
    "            face_keypnts = face_keypnts * (width, height)\n",
    "            face_keypnts = face_keypnts.astype('int')\n",
    " \n",
    "            relevant_keypnts = []\n",
    " \n",
    "            for i in selected_keypoint_indices:\n",
    "                relevant_keypnts.append(face_keypnts[i])\n",
    "            return relevant_keypnts\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filter_img(img_path, has_alpha):\n",
    "    # Read the image\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    " \n",
    "    alpha = None\n",
    "    if has_alpha:\n",
    "        b, g, r, alpha = cv2.split(img)\n",
    "        img = cv2.merge((b, g, r))\n",
    " \n",
    "    return img, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_landmarks(annotation_file):\n",
    "    with open(annotation_file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        points = {}\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            # skip head or empty line if it's there\n",
    "            try:\n",
    "                x, y = int(row[1]), int(row[2])\n",
    "                points[row[0]] = (x, y)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_convex_hull(points):\n",
    "    hull = []\n",
    "    hullIndex = cv2.convexHull(np.array(list(points.values())), clockwise=False, returnPoints=False)\n",
    "    addPoints = [\n",
    "        [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59],  # Outer lips\n",
    "        [60], [61], [62], [63], [64], [65], [66], [67],  # Inner lips\n",
    "        [27], [28], [29], [30], [31], [32], [33], [34], [35],  # Nose\n",
    "        [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47],  # Eyes\n",
    "        [17], [18], [19], [20], [21], [22], [23], [24], [25], [26]  # Eyebrows\n",
    "    ]\n",
    "    hullIndex = np.concatenate((hullIndex, addPoints))\n",
    "    for i in range(0, len(hullIndex)):\n",
    "        hull.append(points[str(hullIndex[i][0])])\n",
    " \n",
    "    return hull, hullIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filter(filter_name=\"dog\"):\n",
    " \n",
    "    filters = filters_config[filter_name]\n",
    " \n",
    "    multi_filter_runtime = []\n",
    " \n",
    "    for filter in filters:\n",
    "        temp_dict = {}\n",
    " \n",
    "        img1, img1_alpha = load_filter_img(filter['path'], filter['has_alpha'])\n",
    " \n",
    "        temp_dict['img'] = img1\n",
    "        temp_dict['img_a'] = img1_alpha\n",
    " \n",
    "        points = load_landmarks(filter['anno_path'])\n",
    " \n",
    "        temp_dict['points'] = points\n",
    " \n",
    "        if filter['morph']:\n",
    "            # Find convex hull for delaunay triangulation using the landmark points\n",
    "            hull, hullIndex = find_convex_hull(points)\n",
    " \n",
    "            # Find Delaunay triangulation for convex hull points\n",
    "            sizeImg1 = img1.shape\n",
    "            rect = (0, 0, sizeImg1[1], sizeImg1[0])\n",
    "            dt = fbc.calculateDelaunayTriangles(rect, hull)\n",
    " \n",
    "            temp_dict['hull'] = hull\n",
    "            temp_dict['hullIndex'] = hullIndex\n",
    "            temp_dict['dt'] = dt\n",
    " \n",
    "            if len(dt) == 0:\n",
    "                continue\n",
    "        if filter['animated']:\n",
    "            filter_cap = cv2.VideoCapture(filter['path'])\n",
    "            temp_dict['cap'] = filter_cap\n",
    "        multi_filter_runtime.append(temp_dict)\n",
    "    return filters, multi_filter_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n",
      "Face not detected!!!\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    " \n",
    "# Some variables we will need later\n",
    "count = 0\n",
    "isFirstFrame = True\n",
    "sigma = 50\n",
    " \n",
    "# Load an initial filter\n",
    "iter_filter_keys = iter(filters_config.keys())\n",
    "filters, multi_filter_runtime = load_filter(next(iter_filter_keys))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    else:\n",
    "        # frame=cv2.imread(\"saif.jpg\")\n",
    "        points2 = getLandmarks(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # if face is partially detected\n",
    "        if not points2 or (len(points2) != 75):\n",
    "            continue\n",
    " \n",
    "        ################ Optical Flow and Stabilization Code #####################\n",
    "        img2Gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if isFirstFrame:\n",
    "            points2Prev = np.array(points2, np.float32)\n",
    "            img2GrayPrev = np.copy(img2Gray)\n",
    "            isFirstFrame = False\n",
    " \n",
    "        lk_params = dict(winSize=(101, 101), maxLevel=15,\n",
    "                         criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.001))\n",
    "        points2Next, st, err = cv2.calcOpticalFlowPyrLK(img2GrayPrev, img2Gray, points2Prev,\n",
    "                                                        np.array(points2, np.float32),\n",
    "                                                        **lk_params)\n",
    " \n",
    "        # Final landmark points are a weighted average of detected landmarks and tracked landmarks\n",
    " \n",
    "        for k in range(0, len(points2)):\n",
    "            d = cv2.norm(np.array(points2[k]) - points2Next[k])\n",
    "            alpha = math.exp(-d * d / sigma)\n",
    "            points2[k] = (1 - alpha) * np.array(points2[k]) + alpha * points2Next[k]\n",
    "            points2[k] = fbc.constrainPoint(points2[k], frame.shape[1], frame.shape[0])\n",
    "            points2[k] = (int(points2[k][0]), int(points2[k][1]))\n",
    " \n",
    "        # Update variables for next pass\n",
    "        points2Prev = np.array(points2, np.float32)\n",
    "        img2GrayPrev = img2Gray\n",
    "        ################ End of Optical Flow and Stabilization Code ###############\n",
    " \n",
    "        # if VISUALIZE_FACE_POINTS:    \n",
    "        #     for idx, point in enumerate(points2):\n",
    "        #         cv2.circle(frame, point, 2, (255, 0, 0), -1)\n",
    "        #         cv2.putText(frame, str(idx), point, cv2.FONT_HERSHEY_SIMPLEX, .3, (255, 255, 255), 1)\n",
    "        #     cv2.imshow(\"landmarks\", frame)\n",
    " \n",
    "        for idx, filter in enumerate(filters):\n",
    "            filter_runtime = multi_filter_runtime[idx]\n",
    "            img1 = filter_runtime['img']\n",
    "            points1 = filter_runtime['points']\n",
    "            img1_alpha = filter_runtime['img_a']\n",
    " \n",
    "            if filter['morph']:\n",
    " \n",
    "                hullIndex = filter_runtime['hullIndex']\n",
    "                dt = filter_runtime['dt']\n",
    "                hull1 = filter_runtime['hull']\n",
    " \n",
    "                # create copy of frame\n",
    "                warped_img = np.copy(frame)\n",
    " \n",
    "                # Find convex hull\n",
    "                hull2 = []\n",
    "                for i in range(0, len(hullIndex)):\n",
    "                    hull2.append(points2[hullIndex[i][0]])\n",
    " \n",
    "                mask1 = np.zeros((warped_img.shape[0], warped_img.shape[1]), dtype=np.float32)\n",
    "                mask1 = cv2.merge((mask1, mask1, mask1))\n",
    "                img1_alpha_mask = cv2.merge((img1_alpha, img1_alpha, img1_alpha))\n",
    " \n",
    "                # Warp the triangles\n",
    "                for i in range(0, len(dt)):\n",
    "                    t1 = []\n",
    "                    t2 = []\n",
    " \n",
    "                    for j in range(0, 3):\n",
    "                        t1.append(hull1[dt[i][j]])\n",
    "                        t2.append(hull2[dt[i][j]])\n",
    " \n",
    "                    fbc.warpTriangle(img1, warped_img, t1, t2)\n",
    "                    fbc.warpTriangle(img1_alpha_mask, mask1, t1, t2)\n",
    " \n",
    "                # Blur the mask before blending\n",
    "                mask1 = cv2.GaussianBlur(mask1, (3, 3), 10)\n",
    " \n",
    "                mask2 = (255.0, 255.0, 255.0) - mask1\n",
    " \n",
    "                # Perform alpha blending of the two images\n",
    "                temp1 = np.multiply(warped_img, (mask1 * (1.0 / 255)))\n",
    "                temp2 = np.multiply(frame, (mask2 * (1.0 / 255)))\n",
    "                output = temp1 + temp2\n",
    "            # else:\n",
    "            #     dst_points = [points2[int(list(points1.keys())[0])], points2[int(list(points1.keys())[1])]]\n",
    "            #     tform = fbc.similarityTransform(list(points1.values()), dst_points)\n",
    "            #     # Apply similarity transform to input image\n",
    "            #     trans_img = cv2.warpAffine(img1, tform, (frame.shape[1], frame.shape[0]))\n",
    "            #     trans_alpha = cv2.warpAffine(img1_alpha, tform, (frame.shape[1], frame.shape[0]))\n",
    "            #     mask1 = cv2.merge((trans_alpha, trans_alpha, trans_alpha))\n",
    " \n",
    "            #     # Blur the mask before blending\n",
    "            #     mask1 = cv2.GaussianBlur(mask1, (3, 3), 10)\n",
    " \n",
    "            #     mask2 = (255.0, 255.0, 255.0) - mask1\n",
    " \n",
    "            #     # Perform alpha blending of the two images\n",
    "            #     temp1 = np.multiply(trans_img, (mask1 * (1.0 / 255)))\n",
    "            #     temp2 = np.multiply(frame, (mask2 * (1.0 / 255)))\n",
    "            #     output = temp1 + temp2\n",
    " \n",
    "            frame = output = np.uint8(output)\n",
    "        # cv2.putText(frame, \"Press F to change filters\", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, .5, (255, 0, 0), 1)\n",
    "        # height, width = output[:2]\n",
    "        # output = cv2.resize(output, (int(width/2), int(height/2)))\n",
    "        cv2.imshow(\"Face Filter\", output)\n",
    "\n",
    "        keypressed = cv2.waitKey(1) & 0xFF\n",
    "        if keypressed == 27:\n",
    "            break\n",
    "        # # Put next filter if 'f' is pressed\n",
    "        # elif keypressed == ord('f'):\n",
    "        #     try:\n",
    "        #         filters, multi_filter_runtime = load_filter(next(iter_filter_keys))\n",
    "        #     except:\n",
    "        #         iter_filter_keys = iter(filters_config.keys())\n",
    "        #         filters, multi_filter_runtime = load_filter(next(iter_filter_keys))\n",
    " \n",
    "        # count += 1\n",
    " \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
